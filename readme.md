# ğŸ§° VisaFriendly Data Engineer Job Scraper

## ğŸŒ Overview
This project is a take-home assessment for the **Data Engineer Intern** position at **VisaFriendly**.  
The system is designed to scrape the latest "Data Engineer" job listings, filter them by H-1B sponsoring companies, and store the structured results in an SQLite database. The entire workflow is automated to run every hour.

---

## ğŸš€ Tech Stack

- **Python 3**
- **SerpAPI** (Google Jobs API)
- **SQLite** (lightweight, SQL-compatible database)
- **pandas** (data transformation)
- **requests** (HTTP API interaction)
- **python-dotenv** (secret management)
- **Windows Task Scheduler** (for hourly automation)

---

## âœ… Features

### ğŸ” Web Scraping
Fetches "Data Engineer" job postings from **Google Jobs** within the **last 1 hour** using SerpAPI.

### ğŸ§  Data Extraction
Each job listing includes:
- âœ… Job Title  
- âœ… Company Name  
- âœ… Location  
- âœ… Job Type (Full-Time, Contract, Internship)  
- âœ… Work Setting (Remote, Hybrid, Onsite)  
- âœ… Job Description  
- âœ… Application Link (if available)  
- âœ… Posting Time  

### ğŸ¯ H-1B Sponsorship Filtering
Filters job listings based on a list of verified **H-1B sponsoring companies** (CSV-based).

### ğŸ—ƒï¸ Structured Database Storage
Filtered jobs are saved to an **SQLite database (`jobs.db`)** using a clean, normalized schema.  
Duplicate prevention is handled via hashing of job attributes or uniqueness constraints.

### ğŸ” Hourly Automation
Scheduled to run every hour using **Windows Task Scheduler**:
- Avoids duplicate entries  
- Continuously fetches new listings  
- Fully autonomous

---

## ğŸ“ Project Structure

```bash
WEB_SCRAPPER/
â”œâ”€â”€ __pycache__/                # Cached Python bytecode
â”œâ”€â”€ .env                        # Contains SerpAPI key (excluded from Git)
â”œâ”€â”€ db_loader.py                # Handles database insertion
â”œâ”€â”€ h1bcompanies_list.csv       # List of H-1B sponsoring companies
â”œâ”€â”€ jobs.db                     # SQLite database file
â”œâ”€â”€ output.csv                  # Optional: Raw scraped job data
â”œâ”€â”€ read_jobs.py                # Utility to read and inspect DB contents
â”œâ”€â”€ readme.md                   # Project documentation (this file)
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ scrape_jobs.py              # Main pipeline: scrape â†’ filter â†’ load
âš™ï¸ Setup Instructions
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/amancodit47/visa_scraper.git
cd visa_scraper/WEB_SCRAPPER
2. Install Python Dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Add SerpAPI Key
Create a .env file in the root directory:

ini
Copy
Edit
SERPAPI_API_KEY=your_api_key_here
Alternatively, you can hardcode it in scrape_jobs.py (not recommended for security reasons).

4. Run the Pipeline Manually
bash
Copy
Edit
python scrape_jobs.py
5. View Stored Jobs
bash
Copy
Edit
python read_jobs.py
ğŸ¤– Setting Up Automation (Windows)
To run the script hourly, set up Windows Task Scheduler:

Open Task Scheduler

Click Create Basic Task

Set Trigger to Daily â†’ Repeat every 1 hour

Under Actions, choose Start a Program

Set Program/script to:

pgsql
Copy
Edit
path\to\python.exe
And Add arguments:

pgsql
Copy
Edit
path\to\scrape_jobs.py
Done! Your scraper is now on autopilot.

ğŸ‘¨â€ğŸ’» Author
Aman

GitHub: @amancodit47

Email: amancodit2004@gmail.com